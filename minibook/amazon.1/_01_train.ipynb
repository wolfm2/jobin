{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.system('ps aux | grep wolfm2')\n",
    "#os.system('killall -s SIGKILL -u wolfm2')\n",
    "#os.system('cp /home/wolfm2/job.sh .; echo test 1>&2') #; cp ../job.log ../jerbb.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read raw training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364000, 14)\n"
     ]
    }
   ],
   "source": [
    "amazon = pd.read_csv('/home/wolfm2/amazon_data/raw_data_train.csv')\n",
    "print(amazon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0  Unnamed: 0.1      Id   ProductId          UserId  \\\n",
      "0       150581        487850  487851  B0025UCD76  A28B2M0XRXHXIG   \n",
      "1       334018         21518   21519  B002QWP89S   A7JJX3KMDZD2F   \n",
      "2        76657        319457  319458  B001GVIUX6  A2S8RJ6DRKGYON   \n",
      "3       357903        248851  248852  B0009JRH1C  A1FLQ698D9C0C8   \n",
      "4       301824        394613  394614  B001B4VOQI  A2KJO9EPX17ZXE   \n",
      "5        59556        501110  501111  B000YVGMZW  A3LULJF2MQSAAQ   \n",
      "6       365559        478948  478949  B001EQ52S4   AYTHZX0M3NFUS   \n",
      "7       429018        555184  555185  B001VNKWD0  A2M069CN0QEW5N   \n",
      "8       342078        458549  458550  B000FFIL92  A2UQY1VSFPNUGU   \n",
      "9       250701         68227   68228  B003EM7J9Q  A28CGE4EOFSSDE   \n",
      "10      190737        411661  411662  B001D6KH8K  A230CR2UUAURZE   \n",
      "11      161823        428775  428776  B0053DJYOC   ALDI31TKPHP2R   \n",
      "12      355008        419638  419639  B0029ZAOW8  A2IJAW6XZXETIA   \n",
      "13      307044        119971  119972  B0002D9YIY  A3SG4TINAHO7TR   \n",
      "14       25537        414984  414985  B005VOOLXM  A126KX6FVI4T66   \n",
      "15      285989        554379  554380  B000JUFH90   AQJVR43GY5LCT   \n",
      "16       81928        491548  491549  B000FFS91M   AI55BYU3DC9NR   \n",
      "17       42561        496012  496013  B008JKU2CO  A2JFUXMTD3VTD9   \n",
      "18      269862        249308  249309  B004TPKAN4  A3JHC8O59WDHFZ   \n",
      "19       88055        339645  339646  B0005YLONI  A2ZOL79SQEO5SZ   \n",
      "20       44375        185705  185706  B003EI7V8O  A2IUWHA2RZ9IHV   \n",
      "21      332715        188431  188432  B001EPPV7C   AEINLNM3JYXGJ   \n",
      "22      430314        146806  146807  B004CH6FN8  A3KTX2QF535YAO   \n",
      "23      141104        221347  221348  B0029XEYJ4  A3955M7LQQAJI2   \n",
      "24      450481        309844  309845  B0060KOI6Q  A235PK7BK2XLC4   \n",
      "25      221775        321968  321969  B0012V1G0Y   A1R9WMKQCVZ3U   \n",
      "26      256051        252022  252023  B001BU0JEK  A2Y0ZT5IIWYLNG   \n",
      "27        3701        497820  497821  B000LKVD5U  A3RB0YBBYGXBYE   \n",
      "28      429397        562226  562227  B004HOSGWE  A1KIW2VOQ9M8F8   \n",
      "29       87306        184643  184644  B001BCVY4W   AAA0TUKS5VBSA   \n",
      "\n",
      "                    ProfileName  HelpfulnessNumerator  HelpfulnessDenominator  \\\n",
      "0                          B622                     0                       0   \n",
      "1   Shinichi Isozaki \"shincyan\"                     1                       2   \n",
      "2                    M. Ronning                     1                       2   \n",
      "3                      G. Zhang                     4                       8   \n",
      "4                     Musical E                     0                       0   \n",
      "5                 Ruben Miranda                     0                       0   \n",
      "6   Leslie Barrows \"Red Ginger\"                     2                       2   \n",
      "7                     Starlette                     1                       1   \n",
      "8            born not to follow                     0                       0   \n",
      "9             NYCGIRL \"NYCGIRL\"                     0                       0   \n",
      "10                      captk53                     1                       1   \n",
      "11                    Elizabeth                     0                       0   \n",
      "12                       Had to                     1                       2   \n",
      "13       MusicalGenius \"Jeremy\"                     5                       7   \n",
      "14  R. Bagula \"Roger L. Bagula\"                     0                       0   \n",
      "15               Ronnie Raymond                     0                       0   \n",
      "16              Boogey Boo \"-S\"                     1                       1   \n",
      "17            L. Annie Foerster                     1                       1   \n",
      "18                        Silea                     0                       1   \n",
      "19                  Paul Keller                     1                       1   \n",
      "20                          ELy                     0                       0   \n",
      "21                 M. Whitehead                     3                       3   \n",
      "22            Jennifer S. Lewis                     0                       0   \n",
      "23              Roberto Cassani                     1                       1   \n",
      "24                      HeyJude                     4                       4   \n",
      "25   indestructible, yea right!                     0                       0   \n",
      "26                        Fdguy                     1                       1   \n",
      "27            Matt C. \"Mattman\"                     0                       0   \n",
      "28                       rhonda                     0                       1   \n",
      "29                   Nerd Alert                     1                       1   \n",
      "\n",
      "    Score        Time                                            Summary  \\\n",
      "0       5  1313020800                                         DELICIOUS!   \n",
      "1       5  1268524800                     The pet dog is delighted, too!   \n",
      "2       2  1313798400  may be healthy but my \"eat anything\" cat won't...   \n",
      "3       5  1255478400                  Weight Loss Benefits of Green Tea   \n",
      "4       5  1305849600                     Healthy High Quality Dog Treat   \n",
      "5       4  1303084800  Says chickenofthesea.com in tiny letters. Tast...   \n",
      "6       5  1278201600                             Addicted to this stuff   \n",
      "7       5  1330214400                                Excellent cinnamon!   \n",
      "8       2  1277942400  NIce presentation but label takes away from th...   \n",
      "9       5  1304985600                                   BRILLIANT SNACK!   \n",
      "10      1  1285027200                          Wintergreen not root beer   \n",
      "11      5  1337644800                                 Surprisingly good!   \n",
      "12      5  1348704000                                       Holey smokes   \n",
      "13      5  1133913600                                  It's Flan-tastic!   \n",
      "14      4  1333152000                                       A good blend   \n",
      "15      5  1269907200                                       great taster   \n",
      "16      3  1251244800                              Decent but not for me   \n",
      "17      4  1272412800                                  better than chips   \n",
      "18      4  1320192000                                    Good, but hard.   \n",
      "19      5  1175040000               Nothing else in a bottle comes close   \n",
      "20      5  1300406400                   Unconventional But Great Product   \n",
      "21      5  1265673600                                          green tea   \n",
      "22      5  1322438400                                     Great product!   \n",
      "23      4  1303862400                                        Un favorito   \n",
      "24      3  1327449600                                              so-so   \n",
      "25      1  1327190400   Just another over priced low performing chew toy   \n",
      "26      5  1336176000                                Great Retail Item!!   \n",
      "27      5  1256083200                               Delicious as always!   \n",
      "28      5  1334188800                  Blue Diamond Almonds Habanero BBQ   \n",
      "29      5  1234224000                                       People Food?   \n",
      "\n",
      "                                                 Text  helpScore  helpful  \\\n",
      "0   This BBQ sauce is DELICIOUS!!  Sweet and tangy...        NaN    False   \n",
      "1   I gave a pet dog plural resemblance products, ...   0.500000    False   \n",
      "2   I tried this in place of Iams.  My hefty maine...   0.500000    False   \n",
      "3   Weight Loss Benefits of Green Tea<br />=======...   0.500000    False   \n",
      "4   Yes, they are a bit expensive but, they are hi...        NaN    False   \n",
      "5   I was hungry and picked up a can. for $1.69 I ...        NaN    False   \n",
      "6   I absolutely adore this stuff. I don't know if...   1.000000    False   \n",
      "7   Now THIS is real cinnamon! Wonderful flavor - ...   1.000000    False   \n",
      "8   I am disappointed with this purchase and I am ...        NaN    False   \n",
      "9   What an awesome snack, super easy to make and ...        NaN    False   \n",
      "10  I agree with the other review here.  Strong wi...   1.000000    False   \n",
      "11  I've been in a bread/tortilla withdrawal since...        NaN    False   \n",
      "12  Let me just tell you what a great decision 5 h...   0.500000    False   \n",
      "13  My girlfriend of 7 years and I recently visite...   0.714286    False   \n",
      "14  This coffee has me looking forward to my break...        NaN    False   \n",
      "15  Fast Shipping and a great tasting flavor,highl...        NaN    False   \n",
      "16  My opinion is that it's a very weak chai.  Har...   1.000000    False   \n",
      "17  When I need a snack I know fruit would be bett...   1.000000    False   \n",
      "18  These are pretty good gingersnaps. They even h...   0.000000    False   \n",
      "19  Just thinking about this stuff makes me hungry...   1.000000    False   \n",
      "20  My dogs loved the product (and they're extreme...        NaN    False   \n",
      "21  I bought this for my son and he was wowed with...   1.000000    False   \n",
      "22  I have ordered from this company before, and I...        NaN    False   \n",
      "23  Muy bueno , bien madurado aunque a punto de pa...   1.000000    False   \n",
      "24  Let me start by saying this is a bit of a solu...   1.000000     True   \n",
      "25  Was really excited to find the Large Everlasti...        NaN    False   \n",
      "26  Nice retro product!!  Surprisingly no one comp...   1.000000    False   \n",
      "27  Oh, I love these chips! And they're so hard to...        NaN    False   \n",
      "28  I love the product, they have great taste, eas...   0.000000    False   \n",
      "29  As this cat food has the somewhat comical tagl...   1.000000    False   \n",
      "\n",
      "                                        summaryFilter  timeFilter  \n",
      "0                                          DELICIOUS!           0  \n",
      "1                      The pet dog is delighted, too!      259200  \n",
      "2   may be healthy but my \"eat anything\" cat won't...      172800  \n",
      "3                   Weight Loss Benefits of Green Tea      518400  \n",
      "4                      Healthy High Quality Dog Treat       86400  \n",
      "5   Says chickenofthesea.com in tiny letters. Tast...      345600  \n",
      "6                              Addicted to this stuff      259200  \n",
      "7                                 Excellent cinnamon!      259200  \n",
      "8   NIce presentation but label takes away from th...           0  \n",
      "9                                    BRILLIANT SNACK!      432000  \n",
      "10                          Wintergreen not root beer      432000  \n",
      "11                                 Surprisingly good!      432000  \n",
      "12                                       Holey smokes           0  \n",
      "13                                  It's Flan-tastic!      518400  \n",
      "14                                       A good blend      172800  \n",
      "15                                       great taster      432000  \n",
      "16                              Decent but not for me      518400  \n",
      "17                                  better than chips      518400  \n",
      "18                                    Good, but hard.      518400  \n",
      "19               Nothing else in a bottle comes close      518400  \n",
      "20                   Unconventional But Great Product       86400  \n",
      "21                                          green tea      432000  \n",
      "22                                     Great product!      345600  \n",
      "23                                        Un favorito      518400  \n",
      "24                                              so-so      518400  \n",
      "25   Just another over priced low performing chew toy      259200  \n",
      "26                                Great Retail Item!!      172800  \n",
      "27                               Delicious as always!      518400  \n",
      "28                  Blue Diamond Almonds Habanero BBQ           0  \n",
      "29                                       People Food?      432000  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.073206043956\n"
     ]
    }
   ],
   "source": [
    "print(amazon.head())\n",
    "print(amazon['helpful'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction on natural language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer = CountVectorizer()\n",
    "# corpus = amazon.Text.as_matrix()\n",
    "# X_bag_of_words = vectorizer.fit_transform(corpus)\n",
    "# print(X_bag_of_words.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('popular')\n",
    "\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vectorize Bag of Words from review text; as sparse matrix\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "#hv = HashingVectorizer(n_features=2 ** 17, non_negative=True)\n",
    "#  analyzer=stemmed_words,\n",
    "\n",
    "# look at the text tokenizer=LemmaTokenizer(), strip_accents=ascii,  stop_words={'english'}, \n",
    "hv0 = HashingVectorizer(n_features=2 ** 19, non_negative=True,\n",
    "                           ngram_range=(1,4)) #, token_pattern = r'\\b[a-zA-Z0-9]{3,}\\b')\n",
    "X_hv0 = hv0.fit_transform(amazon.Text) # mw adds uid as token\n",
    "\n",
    "# # and a second domain where we look at the summary\n",
    "# amazon['summaryFilter'] = amazon['Summary'].apply(lambda x: \" \" if x is np.nan else x) # some were np.nans\n",
    "# hv1 = HashingVectorizer(n_features=2 ** 18, non_negative=True, strip_accents=ascii, tokenizer=LemmaTokenizer(), stop_words={'english'}, \n",
    "#                            ngram_range=(1,3), token_pattern = r'\\b[a-zA-Z0-9]{3,}\\b')\n",
    "# X_hv1 = hv1.fit_transform(amazon.summaryFilter) \n",
    "\n",
    "# Another hash domain we want to count but not scale\n",
    "# amazon['timeFilter'] = amazon['Time'].apply(lambda x: str(int(x)%(86400 * 7))) # converts to day of week\n",
    "# hv2 = HashingVectorizer(n_features=2 ** 17, non_negative=True, strip_accents=ascii, \n",
    "#                            ngram_range=(1,1)) \n",
    "# X_hv2 = hv2.fit_transform(amazon.timeFilter + \" \" + amazon.ProductId + \" \" + amazon.UserId) # mw adds uid as token\n",
    "\n",
    "amazon['ScoreX'] = amazon['Score'].apply(lambda x: str(x))) # converts to day of week\n",
    "hv2 = HashingVectorizer(n_features=2 ** 17, non_negative=True, strip_accents=ascii, \n",
    "                           ngram_range=(1,1)) \n",
    "X_hv2 = hv2.fit_transform(amazon.ScoreX) # mw adds uid as token\n",
    "\n",
    "\n",
    "# hv0 = HashingVectorizer(n_features=2 ** 17, non_negative=True)\n",
    "# X_hv0 = hv0.fit_transform(amazon.ProductId + \" \" + amazon.UserId + \" \" + amazon.Text) # mw adds uid as token\n",
    "\n",
    "# amazon['summaryFilter'] = amazon['Summary'].apply(lambda x: \" \" if x is np.nan else x) # some were np.nans\n",
    "\n",
    "# hv1 = HashingVectorizer(n_features=2 ** 17, non_negative=True)\n",
    "# X_hv1 = hv1.fit_transform(amazon.summaryFilter)\n",
    "\n",
    "#import scipy.sparse as sp\n",
    "#X_hv = sp.hstack([X_hv0, X_hv1], format='csr')\n",
    "X_hv = X_hv0\n",
    "print(X_hv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    A28B2M0XRXHXIG This BBQ sauce is DELICIOUS!!  ...\n",
       "1    A7JJX3KMDZD2F I gave a pet dog plural resembla...\n",
       "2    A2S8RJ6DRKGYON I tried this in place of Iams. ...\n",
       "3    A1FLQ698D9C0C8 Weight Loss Benefits of Green T...\n",
       "4    A2KJO9EPX17ZXE Yes, they are a bit expensive b...\n",
       "5    A3LULJF2MQSAAQ I was hungry and picked up a ca...\n",
       "6    AYTHZX0M3NFUS I absolutely adore this stuff. I...\n",
       "7    A2M069CN0QEW5N Now THIS is real cinnamon! Wond...\n",
       "8    A2UQY1VSFPNUGU I am disappointed with this pur...\n",
       "9    A28CGE4EOFSSDE What an awesome snack, super ea...\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = amazon.UserId + \" \" +  amazon.Text\n",
    "# x.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hv.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to be able to use this model fit on other data (the test set)\n",
    "# So let's save a copy of this instance of HashingVectorizer to be able to transform other data with this fit\n",
    "# http://scikit-learn.org/stable/modules/model_persistence.html\n",
    "joblib.dump(hv0, 'hv0.pkl') # pickle\n",
    "# joblib.dump(hv1, 'hv1.pkl') # pickle\n",
    "joblib.dump(hv2, 'hv2.pkl') # pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "X_tfidf = transformer.fit_transform(X_hv)\n",
    "\n",
    "joblib.dump(transformer, 'transformer.pkl') # pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create additional quantitative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Score  reviewLen\n",
      "0      5        110\n",
      "1      5        140\n",
      "2      2        471\n",
      "3      5      10800\n",
      "4      5        152\n",
      "5      4        231\n",
      "6      5        271\n",
      "7      5        320\n",
      "8      2        362\n",
      "9      5        283\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# features from Amazon.csv to add to feature set\n",
    "#amazon['reviewLen'] = amazon['Text'].str.len()\n",
    "#amazon['summaryLen'] = amazon['summaryFilter'].str.len()\n",
    "\n",
    "#amazon['rlMeanDist'] = amazon['reviewLen'].apply(lambda x: abs(x-80)) # 80 is avg summary len. Thx George!\n",
    "#amazon['slMeanDist'] = amazon['summaryLen'].apply(lambda x: abs(x-8)) # 8. just guessing here.\n",
    "\n",
    "#import zlib\n",
    "#amazon['nameHash'] = zlib.crc32(str(amazon['UserId']).encode('utf8'))\n",
    "#amazon['nameHash'] = amazon['UserId'].apply(lambda x: zlib.crc32(str(x).encode('utf8'))) # bad. don't do it this way\n",
    "\n",
    "# X_quant_features = amazon[[\"Score\", \"reviewLen\", \"summaryLen\", \"rlMeanDist\", \"slMeanDist\"]]\n",
    "# print(X_quant_features.head(10))\n",
    "# print(type(X_quant_features))\n",
    "X_quant_features = amazon[[]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all quantitative features into a single sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "X_quant_features_csr = csr_matrix(X_quant_features)\n",
    "X_combined = hstack([X_tfidf, X_quant_features_csr, X_hv2])  # we dont want to penalize hv2 w tfidf MW\n",
    "X_matrix = csr_matrix(X_combined) # convert to sparse matrix\n",
    "print(X_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `X`, scaled matrix of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364000, 131074)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sc.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler(with_mean=False)\n",
    "X = sc.fit_transform(X_matrix)\n",
    "print(X.shape)\n",
    "\n",
    "joblib.dump(sc, 'sc.pkl') # pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create `y`, vector of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "y = amazon['helpful'].values\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from my_measures import BinaryClassificationPerformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronhill/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pos': 26582, 'Neg': 337418, 'TP': 11850, 'TN': 325157, 'FP': 12261, 'FN': 14732, 'Accuracy': 0.92584340659340658, 'Precision': 0.49147691924847581, 'Recall': 0.44579038447069447, 'desc': 'svm'}\n"
     ]
    }
   ],
   "source": [
    "# # MODEL: SVM, linear\n",
    "# from sklearn import linear_model\n",
    "# svm = linear_model.SGDClassifier()\n",
    "# svm.fit(X, y)\n",
    "# joblib.dump(svm, 'svm.pkl') # pickle\n",
    "\n",
    "# svm_performance = BinaryClassificationPerformance(svm.predict(X), y, 'svm')\n",
    "# svm_performance.compute_measures()\n",
    "# print(svm_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronhill/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pos': 26582, 'Neg': 337418, 'TP': 13479, 'TN': 324641, 'FP': 12777, 'FN': 13103, 'Accuracy': 0.92890109890109895, 'Precision': 0.51336837294332727, 'Recall': 0.50707245504476717, 'desc': 'lgs'}\n"
     ]
    }
   ],
   "source": [
    "# # MODEL: logistic regression\n",
    "# from sklearn import linear_model\n",
    "# #lgs = linear_model.SGDClassifier(loss='log', n_iter=50, alpha=0.00001)\n",
    "# lgs = linear_model.SGDClassifier(loss='log', n_iter=1000, alpha=0.1)\n",
    "\n",
    "# lgs.fit(X, y)\n",
    "# joblib.dump(lgs, 'lgs.pkl') # pickle\n",
    "\n",
    "# lgs_performance = BinaryClassificationPerformance(lgs.predict(X), y, 'lgs')\n",
    "# lgs_performance.compute_measures()\n",
    "# print(lgs_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pos': 26582, 'Neg': 337418, 'TP': 17202, 'TN': 295255, 'FP': 42163, 'FN': 9380, 'Accuracy': 0.85839835164835165, 'Precision': 0.28976669754906087, 'Recall': 0.64712963659619294, 'desc': 'nbs'}\n"
     ]
    }
   ],
   "source": [
    "# # MODEL: Naive Bayes\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# nbs = MultinomialNB()\n",
    "# nbs.fit(X, y)\n",
    "# joblib.dump(nbs, 'nbs.pkl') # pickle\n",
    "\n",
    "# nbs_performance = BinaryClassificationPerformance(nbs.predict(X), y, 'nbs')\n",
    "# nbs_performance.compute_measures()\n",
    "# print(nbs_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pos': 26582, 'Neg': 337418, 'TP': 7923, 'TN': 336780, 'FP': 638, 'FN': 18659, 'Accuracy': 0.94698626373626371, 'Precision': 0.92547599579488382, 'Recall': 0.29805883680686179, 'desc': 'rdg'}\n"
     ]
    }
   ],
   "source": [
    "# # MODEL: Ridge Regression Classifier\n",
    "# from sklearn import linear_model\n",
    "# rdg = linear_model.RidgeClassifier()\n",
    "# rdg.fit(X, y)\n",
    "# joblib.dump(rdg, 'rdg.pkl') # pickle\n",
    "\n",
    "# rdg_performance = BinaryClassificationPerformance(rdg.predict(X), y, 'rdg')\n",
    "# rdg_performance.compute_measures()\n",
    "# print(rdg_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronhill/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pos': 26582, 'Neg': 337418, 'TP': 11895, 'TN': 323880, 'FP': 13538, 'FN': 14687, 'Accuracy': 0.92245879120879126, 'Precision': 0.46769944560217042, 'Recall': 0.44748325934843125, 'desc': 'prc'}\n"
     ]
    }
   ],
   "source": [
    "# # MODEL: Perceptron\n",
    "# from sklearn import linear_model\n",
    "# prc = linear_model.SGDClassifier(loss='perceptron')\n",
    "# prc.fit(X, y)\n",
    "# joblib.dump(prc, 'prc.pkl') # pickle\n",
    "\n",
    "# prc_performance = BinaryClassificationPerformance(prc.predict(X), y, 'prc')\n",
    "# prc_performance.compute_measures()\n",
    "# print(prc_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier # mw\n",
    "\n",
    "# prepare a range of alpha values to test\n",
    "# alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "alphas = np.array([1, 0.1, 0.01, 0.001, 0.0001, 0.00001])\n",
    "Cs = np.array([0.001, 0.01, 0.1, 1, 10, 100, 1000])\n",
    "# create and fit a ridge regression model, testing each alpha\n",
    "# model = linear_model.SGDClassifier(loss='perceptron', max_iter=50) # max_iter 1000\n",
    "\n",
    "mlp = MLPClassifier(random_state=0)\n",
    "svm = linear_model.SGDClassifier(n_iter=500)\n",
    "lgs = linear_model.SGDClassifier(loss='log', n_iter=500)\n",
    "nbs = MultinomialNB()\n",
    "rdg = linear_model.RidgeClassifier()\n",
    "prc = linear_model.SGDClassifier(loss='perceptron', n_iter=500)\n",
    "\n",
    "for model in [[svm,\"svm\"], [lgs,\"lgs\"], [prc,\"prc\"], [nbs,\"nbs\"], [rdg,\"rdg\"]]: \n",
    "# for model in []: \n",
    "# for model in [rdg]:    \n",
    "  fh = open(\"GridSearch.txt\", \"a\")\n",
    "  grid = GridSearchCV(estimator=model[0], param_grid=dict(alpha=alphas), n_jobs=2) #\n",
    "  grid.fit(X, y)\n",
    "  print(grid)\n",
    "  # summarize the results of the grid search\n",
    "  print(grid.cv_results_)\n",
    "  print(grid.best_score_)\n",
    "  print(grid.best_estimator_.alpha)\n",
    "\n",
    "  fh.write('\\n########\\n')\n",
    "  fh.write(str(datetime.datetime.now()))\n",
    "  fh.write('\\n########\\n')\n",
    "  fh.write(str(model[0]) + '\\n')  \n",
    "  fh.write(str(grid.cv_results_).replace(\", '\", \",\\n'\") + '\\n')\n",
    "  fh.write(str(grid.best_score_) + '\\n')  \n",
    "  fh.write(str(grid.best_estimator_.alpha) + '\\n')\n",
    "  fh.close()\n",
    "\n",
    "  # MODEL: BEST\n",
    "  best = grid.best_estimator_\n",
    "\n",
    "  best.fit(X, y)\n",
    "  joblib.dump(best, 'best.{}.pkl'.format(model[1])) # pickle\n",
    "\n",
    "  best_performance = BinaryClassificationPerformance(best.predict(X), y, 'best')\n",
    "  best_performance.compute_measures()\n",
    "  print(best_performance.performance_measures)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "pg = {'learning_rate': [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "'hidden_layer_sizes': [(100,1), (100,2), (100,3)],\n",
    "#'alpha': [10.0 ** -np.arange(1, 7)],\n",
    "'alpha': [1, 0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
    "'activation': [\"logistic\", \"relu\", \"Tanh\"],\n",
    "'tol': [1e-2, 1e-4, 1e-6],\n",
    "'epsilon': [1e-3, 1e-7, 1e-8, 1e-9, 1e-8]\n",
    "}\n",
    "\n",
    "fh = open(\"GridSearch.txt\", \"a\")\n",
    "grid = GridSearchCV(estimator=mlp, param_grid=pg, n_jobs=2) #\n",
    "grid.fit(X, y)\n",
    "print(grid)\n",
    "# summarize the results of the grid search\n",
    "print(grid.cv_results_)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)\n",
    "\n",
    "fh.write('\\n########\\n')\n",
    "fh.write(str(datetime.datetime.now()))\n",
    "fh.write('\\n########\\n')\n",
    "fh.write(str(model) + '\\n')  \n",
    "fh.write(str(grid.cv_results_).replace(\", '\", \",\\n'\") + '\\n')\n",
    "fh.write(str(grid.best_score_) + '\\n')  \n",
    "fh.write(str(grid.best_estimator_.alpha) + '\\n')\n",
    "fh.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# MODEL: BEST\n",
    "best = grid.best_estimator_\n",
    "\n",
    "best.fit(X, y)\n",
    "joblib.dump(best, 'best.pkl') # pickle\n",
    "\n",
    "best_performance = BinaryClassificationPerformance(best.predict(X), y, 'best')\n",
    "best_performance.compute_measures()\n",
    "print(best_performance.performance_measures)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC plot to compare performance of various models and fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x150fd626d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #fits = [svm_performance, lgs_performance, nbs_performance, rdg_performance, prc_performance]\n",
    "# fits = [svm_performance, lgs_performance, rdg_performance, prc_performance]\n",
    "\n",
    "# for fit in fits:\n",
    "#     plt.plot(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "#              fit.performance_measures['TP'] / fit.performance_measures['Pos'], 'ro')\n",
    "#     plt.text(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "#              fit.performance_measures['TP'] / fit.performance_measures['Pos'], fit.desc)\n",
    "# plt.axis([0, 1, 0, 1])\n",
    "# plt.title('ROC plot: training set')\n",
    "# plt.xlabel('False positive rate')\n",
    "# plt.ylabel('True positive rate')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
